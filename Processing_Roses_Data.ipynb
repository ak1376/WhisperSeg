{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperSeg Application to Rose's Canary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ananya Kapoor\n",
    "\n",
    "\n",
    "Forked from https://github.com/nianlonggu/WhisperSeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install the required packages. This isn't an exhaustive list. For the exhaustive list, please refer to \"requirements.txt\" in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (4.42.3)\n",
      "Requirement already satisfied: ctranslate2 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: ipywidgets in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (4.65.0)\n",
      "Requirement already satisfied: scipy in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (1.23.5)\n",
      "Requirement already satisfied: librosa in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (0.10.0.post2)\n",
      "Requirement already satisfied: matplotlib in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: filelock in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: setuptools in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ctranslate2) (66.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipywidgets) (8.12.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: pooch<1.7,>=1.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (0.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: appnope in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pickleshare in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: backcall in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /Users/ananyakapoor/opt/anaconda3/envs/Canary_Deep_Learning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers ctranslate2 ipywidgets tqdm scipy numpy librosa matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from model import WhisperSegmenterFast\n",
    "from audio_utils import WhisperSegFeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we will define the class that will be responsible for the processing of audio files. The main functions of this class are as follows:\n",
    "\n",
    " 1. segment_song: This will apply WhisperSeg to the audio file and will return the samples in the raw audio that correspond to song.\n",
    "\n",
    " 2. silencer: The samples of the raw wav file that correspond to noise (detected from the segment_song function above) will be replaced with silences. This returns a wav file with the noise replaced with silence\n",
    " \n",
    " 3. create_sonogram: create a sonogram of wav file returned from \"silencer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter: \n",
    "    def __init__(self, sr, segmenter, feature_extractor, min_frequency, spec_time_step, min_segment_length, eps, num_trials, wavfiles) -> None:\n",
    "        self.sr = sr\n",
    "        self.segmenter = segmenter\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.min_frequency = min_frequency\n",
    "        self.spec_time_step = spec_time_step\n",
    "        self.min_segment_length = min_segment_length\n",
    "        self.eps = eps\n",
    "        self.num_trials = num_trials\n",
    "\n",
    "        self.wavfiles = wavfiles\n",
    "\n",
    "        self.onset_list = []\n",
    "        self.offset_list = []\n",
    "\n",
    "    def segment_song(self, wavfile_path):\n",
    "\n",
    "        onset_list = []\n",
    "        offset_list = []\n",
    "\n",
    "        audio, _ = librosa.load(wavfile_path, sr = self.sr)\n",
    "\n",
    "        prediction = self.segmenter.segment(audio, sr = sr, min_frequency = self.min_frequency, spec_time_step = self.spec_time_step,\n",
    "                        min_segment_length = self.min_segment_length, eps = self.eps, num_trials = self.num_trials)\n",
    "        \n",
    "        onset_list.append(prediction['onset'])\n",
    "        offset_list.append(prediction['offset'])\n",
    "\n",
    "        return self.sr*np.array(onset_list), self.sr*np.array(offset_list)\n",
    "\n",
    "    \n",
    "    def silencer(self, wavfile, samples_onsets, samples_offsets):\n",
    "        '''\n",
    "        This function will take a noisy wavfile and find the samples according to the samples_onsets and samples_offsets arrays. I will then replace all samples outside of these regions with zeros (silences)\n",
    "        '''\n",
    "\n",
    "        # Flatten the arrays to ensure they are one-dimensional\n",
    "        samples_onsets = np.array(samples_onsets).flatten()\n",
    "        samples_offsets = np.array(samples_offsets).flatten()\n",
    "\n",
    "        # Create an array of silence with the same shape and data type as the input wavfile\n",
    "        silenced_audio = np.zeros_like(wavfile)\n",
    "\n",
    "        # Copy detected regions to the silenced array\n",
    "        for start, end in zip(samples_onsets, samples_offsets):\n",
    "            # Explicitly convert start and end to integers to avoid indexing errors\n",
    "            start_idx = int(start)\n",
    "            end_idx = int(end)\n",
    "            silenced_audio[start_idx:end_idx] = wavfile[start_idx:end_idx]\n",
    "\n",
    "        return silenced_audio\n",
    "    \n",
    "    def create_sonogram(self, audio):\n",
    "        sonogram = self.feature_extractor(audio, sampling_rate=self.sr, padding = \"do_not_pad\" )[\"input_features\"][0]\n",
    "\n",
    "        return sonogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this class to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of object\n",
    "sr = 32000\n",
    "min_frequency = 0\n",
    "spec_time_step = 0.001\n",
    "min_segment_length = 0.005\n",
    "eps = 0.01\n",
    "num_trials = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the version of WhisperSeg finetuned on canaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "segmenter = WhisperSegmenterFast( \"nccratliri/whisperseg-canary-ct2\", device=\"cuda\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More parameter initialization (for more details refer to the readme). We will also define the WhisperSeg object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values\n",
    "window_size = 15\n",
    "spec_width = 1000\n",
    "min_frequency = 0\n",
    "max_frequency = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperSegFeatureExtractor(sr, window_size / spec_width, min_frequency, max_frequency )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rose's data is structured first by Bird ID and then by day of data acquisition. We will extract the bird identities and the days of acquisition  and then apply WhisperSeg to all the wav files under each subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_paths = '/home/akapoor/Desktop/USA5207' # This can be extended to be a list of birds (e.g. USA5207, USA5323, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 15/410 [00:35<15:45,  2.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m seg_jawn\u001b[38;5;241m.\u001b[39mwavfiles[i]\n\u001b[1;32m     40\u001b[0m audio, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path, sr \u001b[38;5;241m=\u001b[39m sr )\n\u001b[0;32m---> 41\u001b[0m sample_onsets, sample_offsets \u001b[38;5;241m=\u001b[39m seg_jawn\u001b[38;5;241m.\u001b[39msegment_song(audio_path)\n\u001b[1;32m     42\u001b[0m silenced_audio \u001b[38;5;241m=\u001b[39m seg_jawn\u001b[38;5;241m.\u001b[39msilencer(audio, sample_onsets, sample_offsets)\n\u001b[1;32m     44\u001b[0m orig_spec \u001b[38;5;241m=\u001b[39m seg_jawn\u001b[38;5;241m.\u001b[39mcreate_sonogram(audio)\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mSegmenter.segment_song\u001b[0;34m(self, wavfile_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m offset_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m audio, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(wavfile_path, sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msr)\n\u001b[0;32m---> 24\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmenter\u001b[38;5;241m.\u001b[39msegment(audio, sr \u001b[38;5;241m=\u001b[39m sr, min_frequency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_frequency, spec_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec_time_step,\n\u001b[1;32m     25\u001b[0m                 min_segment_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_segment_length, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps, num_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_trials)\n\u001b[1;32m     27\u001b[0m onset_list\u001b[38;5;241m.\u001b[39mappend(prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monset\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m offset_list\u001b[38;5;241m.\u001b[39mappend(prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/Canary_Deep_Learning/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Dropbox (University of Oregon)/WhisperSeg/model.py:422\u001b[0m, in \u001b[0;36mSegmenterBase.segment\u001b[0;34m(self, audio, sr, min_frequency, spec_time_step, min_segment_length, eps, time_per_frame_for_voting, consolidation_method, max_length, batch_size, num_trials, num_beams, top_k, top_p, length_penalty, status_monitor)\u001b[0m\n\u001b[1;32m    419\u001b[0m     time_per_frame_for_voting \u001b[38;5;241m=\u001b[39m spec_time_step\n\u001b[1;32m    421\u001b[0m sliced_audios_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sliced_audios_features( audio, sr, min_frequency, spec_time_step, num_trials)\n\u001b[0;32m--> 422\u001b[0m generated_text_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_segment_text( sliced_audios_features, batch_size, max_length, num_beams, top_k, top_p, length_penalty, status_monitor )\n\u001b[1;32m    423\u001b[0m final_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_generation( \n\u001b[1;32m    424\u001b[0m     generated_text_list, sliced_audios_features,\n\u001b[1;32m    425\u001b[0m                  min_segment_length, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m                  consolidation_method \n\u001b[1;32m    431\u001b[0m                 )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_prediction\n",
      "File \u001b[0;32m~/Dropbox (University of Oregon)/WhisperSeg/model.py:178\u001b[0m, in \u001b[0;36mSegmenterBase.generate_segment_text\u001b[0;34m(self, sliced_audios_features, batch_size, max_length, num_beams, top_k, top_p, length_penalty, status_monitor)\u001b[0m\n\u001b[1;32m    176\u001b[0m     all_threads\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_threads:\n\u001b[0;32m--> 178\u001b[0m     t\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    180\u001b[0m generated_text_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m( \u001b[38;5;28mlist\u001b[39m( generated_texts_dict\u001b[38;5;241m.\u001b[39mkeys() ) ):\n",
      "File \u001b[0;32m~/anaconda3/envs/Canary_Deep_Learning/lib/python3.11/threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/Canary_Deep_Learning/lib/python3.11/threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[1;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bird_filepaths = [] # Store each day subfolder\n",
    "\n",
    "for filename in os.listdir(bird_paths):\n",
    "    # if filename == '.DS_Store':\n",
    "    #     continue\n",
    "    file_path = os.path.join(bird_paths, filename)\n",
    "    bird_filepaths.append(file_path)\n",
    "\n",
    "for bird_path in bird_filepaths:\n",
    "    all_days = bird_path\n",
    "\n",
    "    day_filepaths = [] # Will contain the full filepaths of each raw wav file in each day subfolder. \n",
    "\n",
    "    for filename in os.listdir(all_days):\n",
    "        file_path = os.path.join(all_days, filename)\n",
    "        if os.path.isfile(file_path):  # Check if it's a file. I do not want to store any subfolders like \"corrected_mat\", etc.\n",
    "            day_filepaths.append(file_path)\n",
    "    \n",
    "    # Extract bird name\n",
    "    # Split the path by '/'\n",
    "    parts = all_days.split('/')\n",
    "\n",
    "    # Get the last element of the list\n",
    "    bird_name = parts[-1]\n",
    "    \n",
    "    # Create a PNG folder and WAV folder for each day subfolder. \n",
    "    os.makedirs(f'png_files/{bird_name}', exist_ok=True)\n",
    "    os.makedirs(f'new_wav_files/{bird_name}', exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Define the segmentation object \n",
    "    seg_jawn = Segmenter(sr = sr, segmenter = segmenter, feature_extractor=feature_extractor, min_frequency=min_frequency, spec_time_step=spec_time_step, min_segment_length= min_segment_length, eps = eps, num_trials=num_trials, wavfiles= day_filepaths)\n",
    "\n",
    "    # For each wav file we will do the following:\n",
    "    # 1. Segment the raw wav file into song and not song\n",
    "    # 2. Assign the regions corresponding to not song as silences\n",
    "    # 3. Create a sonogram for both the raw wav file and the wav file where \"not song\" is replaced with silences. \n",
    "    for i in tqdm(np.arange(len(seg_jawn.wavfiles))):\n",
    "        audio_path = seg_jawn.wavfiles[i]\n",
    "        audio, _ = librosa.load(audio_path, sr = sr )\n",
    "        sample_onsets, sample_offsets = seg_jawn.segment_song(audio_path)\n",
    "        silenced_audio = seg_jawn.silencer(audio, sample_onsets, sample_offsets)\n",
    "\n",
    "        orig_spec = seg_jawn.create_sonogram(audio)\n",
    "        silenced_spec = seg_jawn.create_sonogram(silenced_audio)\n",
    "\n",
    "        # Plotting (for every 50th wav file)\n",
    "\n",
    "        if i%50 == 0:\n",
    "\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "            # Original audio spectrogram\n",
    "            axes[0].imshow(orig_spec, origin='lower', cmap='viridis')\n",
    "            axes[0].set_title('Original Audio Spectrogram')\n",
    "            axes[0].set_xlabel('Time')\n",
    "            axes[0].set_ylabel('Frequency Bin')\n",
    "\n",
    "            # Silenced audio spectrogram\n",
    "            axes[1].imshow(silenced_spec, origin='lower', cmap='viridis')\n",
    "            axes[1].set_title('Spectrogram from Audio with Noise Removed')\n",
    "            axes[1].set_xlabel('Time')\n",
    "            axes[1].set_ylabel('Frequency Bin')\n",
    "\n",
    "            # Display the plot\n",
    "            plt.tight_layout()\n",
    "\n",
    "            parts = all_days.split('/')\n",
    "            day_value = parts[-1]\n",
    "\n",
    "            os.makedirs(f'png_files/{bird_name}/day_{day_value}', exist_ok=True)\n",
    "\n",
    "            # Split the path by '/'\n",
    "            parts = audio_path.split('/')\n",
    "\n",
    "            # Get the last element of the list\n",
    "            last_part = parts[-1]\n",
    "\n",
    "            song_name = last_part.replace('.wav', '.png')\n",
    "\n",
    "            plt.savefig(f'png_files/{bird_name}/day_{day_value}/{song_name}')\n",
    "            plt.close()\n",
    "\n",
    "        \n",
    "        # Split the path by '/'\n",
    "        parts = audio_path.split('/')\n",
    "\n",
    "        # Get the last element of the list\n",
    "        last_part = parts[-1]\n",
    "\n",
    "        os.makedirs(f'new_wav_files/{bird_name}/day_{day_value}', exist_ok=True)\n",
    "\n",
    "        write(f'new_wav_files/{bird_name}/day_{day_value}/{last_part}', sr, silenced_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Canary_Deep_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
